{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './hdt-tagged/hdt-1-10000-train.tags'\n",
    "test_file_path = './hdt-tagged/hdt-10001-12000-test.tags'\n",
    "\n",
    "\n",
    "def parse_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        read_data = file.read()\n",
    "    # We want a numpy array because it's computational faster. First we lower the string and then we split it.\n",
    "    lines = np.array(read_data.lower().split('\\n'))\n",
    "    # Let's split the lines again this time the seperator is \\t\n",
    "    data2d = list(map(lambda d: d.split('\\t'),lines))\n",
    "    # Filter out each single column row\n",
    "    data2d = np.array(list(filter(lambda w: len(w)>1,data2d)))\n",
    "    # Now we can access the 2d array for the words\n",
    "    words = data2d[:,0]\n",
    "    # and the tags\n",
    "    tags = data2d[:,1]\n",
    "    # return everything\n",
    "    return lines, words, tags\n",
    "# Hope i don't need to explain what this function does :D\n",
    "def get_most_common_tag(word:str,  print_text = True)->str:\n",
    "    # using the tags and words as a global variable because of speed\n",
    "    global tags, words, most_common_tag_in_whole_text\n",
    "    # Hey Siri, give me all the indices where my word appears\n",
    "    word_indices = np.where(words==word.lower())\n",
    "    # Fallback if word appears only once\n",
    "    if len(word_indices[0]) == 1:\n",
    "        if print_text: print('No other words found. Taking most common Tag: {}'.format(most_common_tag_in_whole_text))\n",
    "        return most_common_tag_in_whole_text\n",
    "    # Hey Google, give me all the tags for that word\n",
    "    filtered_tags = tags[word_indices]\n",
    "    # Hey Alexa, give me all the counts for each unique tag\n",
    "    unique_tags, tag_count = np.unique(filtered_tags, return_counts=True)\n",
    "    # Hey Cortana, what is the index of the most counted Tag?\n",
    "    max_idx = np.argmax(tag_count)\n",
    "    # Hey Bixby, Give me the most common tag and it's probability\n",
    "    probabilities = tag_count/tag_count.sum()\n",
    "    most_common_tag = unique_tags[max_idx]\n",
    "    if print_text: print('Hello Matin, The most common Tag for \"{}\" is {} with a probability of {:.2%}'.format(word,most_common_tag.upper(),probabilities[max_idx]))\n",
    "    return most_common_tag\n",
    "\n",
    "def get_most_common_next_tag(tag:str , print_text = True)->str:\n",
    "    # using the tags and words as a global variable because of speed\n",
    "    global tags, words\n",
    "    # Give me all the indices of the next tag\n",
    "    next_indices = np.where(tags==tag)[0]+1\n",
    "    # Remove the last index to not get out of bounds\n",
    "    next_indices = next_indices[:-1]\n",
    "    next_tags = tags[next_indices]\n",
    "    #  Give me all the counts for each unique next tag\n",
    "    next_tags_unique, next_tags_counts = np.unique(next_tags, return_counts=True)\n",
    "    # Hey Siri, what is the most counted next Tag and it's probability\n",
    "    max_idx = np.argmax(next_tags_counts)\n",
    "    probabilities = next_tags_counts/next_tags_counts.sum()\n",
    "    most_common_next_tag = next_tags_unique[max_idx]\n",
    "    if print_text:\n",
    "        print('The most common Tag after \"{}\" is {} with a probability of {:.2%}'.format(tag.upper(),most_common_next_tag.upper(),probabilities[max_idx]))\n",
    "    return most_common_next_tag\n",
    "\n",
    "def test_data():\n",
    "    global test_words, most_common_tag_in_whole_text\n",
    "    cache_words = {}\n",
    "    cache_tags = {}\n",
    "    result_tags = []\n",
    "    for test_word in test_words:\n",
    "        if test_word in cache_words:\n",
    "            result_tags.append(cache_words[test_word])\n",
    "            continue\n",
    "            \n",
    "        # Again a fallback\n",
    "        try:\n",
    "            tag_of_test_word = get_most_common_tag(test_word , print_text =False)\n",
    "            if tag_of_test_word in cache_tags:\n",
    "                result_tags.append(cache_tags[tag_of_test_word])\n",
    "                continue\n",
    "            \n",
    "            next_tag = get_most_common_next_tag(tag_of_test_word,print_text =False )\n",
    "            cache_tags[tag_of_test_word] = next_tag\n",
    "            result_tags.append(next_tag)\n",
    "        except:\n",
    "            result_tags.append(most_common_tag_in_whole_text)\n",
    "        \n",
    "    return result_tags\n",
    "\n",
    "# Let the games begin\n",
    "#First parse Train and test file\n",
    "lines, words, tags= parse_file(file_path)\n",
    "test_lines, test_words, test_tags= parse_file(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the most common words and most common tags\n",
    "unique_words, words_count = np.unique(words, return_counts=True)\n",
    "max_idx = np.argmax(words_count)\n",
    "most_common_word_in_whole_text = unique_words[max_idx]\n",
    "unique_tags, tags_count = np.unique(tags, return_counts=True)\n",
    "max_idx = np.argmax(tags_count)\n",
    "most_common_tag_in_whole_text = unique_tags[max_idx]\n",
    "# We need this as a fallback if a word only appears once..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Matin, The most common Tag for \"das\" is ART with a probability of 84.82%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'art'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_common_tag('das')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common Tag after \"nn\" is APPR with a probability of 19.11%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'appr'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_common_next_tag('nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing data... sorry very slow. Might take 5 Minutes\")\n",
    "result_tags = test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = np.sum(test_tags == result_tags)/ len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The success rate is {:.2%}'.format(success_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
